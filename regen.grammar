// Header
"src/sdk/grammar.rs.pp";

context "Context";

/*===================================
  Tokenizer
===================================*/

// Token types are defined with the token keyword
token Keyword;
token Identifier;
token RegExp;
token Literal;
token Symbol;

// The extract keyword will exclude the token type when parsing the AST
extract token Comment;

// Ignore whitespaces
ignore /\s+/;

// Comment
Comment /\/\/[^\n]*\n?/;
Comment /\/\*([^\*]|(\*[^\/]))*\*\//;

// Literal and RegExp, which are surrounded by either "" or //
Literal /"((\\.)|[^\\"])*"/;
RegExp /\/((\\.)|[^\\\/])*\//;

// Keywords
Keyword "ignore";
Keyword "extract";
Keyword "token";
Keyword "semantic";
Keyword "rule";
Keyword "optional";
Keyword "context";

// Special Symbols
Symbol /[{};|()=,:\.\[\]]/;

// Identifier is alphanumeric and underscore, but does not start with digit
Identifier /[_a-zA-Z]\w*/;

/*===================================
  Semantics
===================================*/
// Semantic types are used to augment the token types.
// For example, the token type "Identifier" can both be a rule name or a variable name
// When defining the rules, you can override the semantic type of a token
// or an entire subtree.

// Semantics
semantic Variable;
semantic Token;
semantic Semantic;
semantic Rule;
semantic HookName;
semantic HookType;
semantic ContextType;

/*===================================
  Rules
===================================*/
// There are 2 types of rules:
// Unions: rule A = B | C;
// Functional: rule A(...) <body>;

// The first rule is the target of the parser.
// The parser can be configured to generate one root node,
// or keep generating root nodes until the end of the file.

rule TopLevelStatement(body: TopLevelDefine, _: token Symbol";") body;

rule TopLevelDefine = 
  TokenLiteral
  | DefineContextStatement
  | DefineRuleStatement
  | DefineTokenTypeStatement
  | DefineIgnoreTokenRuleStatement
  | DefineTokenRuleStatement
  | DefineSemanticStatement;

rule ("parse_context":"()")
DefineContextStatement(
    _: token Keyword"context",
    (ContextType) contextType: token Literal
) contextType;

rule ("parse_rule":"Rule")
DefineRuleStatement(
    _: token Keyword"rule",
    hookAttr: optional HookAttribute,
    (Rule) ruleName: token Identifier,
    body: RuleDefineBody
) {
    hookAttr, ruleName, body
};

rule ("parse_hook":"Hook")
HookAttribute(
    _: token Symbol"(",
    (HookName) hookName: token Literal,
    _: token Symbol":",
    (HookType) hookType: token Literal,
    _: token Symbol")"
) {
    hookName, hookType
};

rule ("parse_rule_value":"RuleValue")
RuleDefineBody = 
  UnionRuleBody | 
  FunctionalRuleBody;

rule UnionRuleBody(
    _: token Symbol"=",
    rules: optional UnionRuleList
) rules;

rule UnionRuleList((Rule) first: token Identifier, rest: optional UnionRuleListTail) first | rest;
rule UnionRuleListTail(
    _: token Symbol"|",
    (Rule) first: token Identifier,
    rest: optional UnionRuleListTail
) first | rest;


rule FunctionalRuleBody(
    _: token Symbol"(",
    params: optional ParamList,
    _: token Symbol")",
    body: optional Expression
) {
    params, body
};


rule ("parse_param_list":"Vec<Param>") ParamList(first: Parameter, rest: optional ParamListTail) first | rest;
rule ParamListTail(_: token Symbol",", first: Parameter, rest: optional ParamListTail) first | rest;

rule ("parse_param":"Param") Parameter(
    semAttr: optional ParamSemantic,
    (Variable) variable: token Identifier,
    _: token Symbol":",
    type: optional RuleType
) {
    semAttr, variable, type
};

rule ParamSemantic(
    _: token Symbol"(",
    (Semantic) semanticName: optional token Identifier,
    _: token Symbol")"
) semanticName; 

rule RuleType(
    kwOptional: optional token Keyword"optional",
    kwToken: optional token Keyword"token",
    id: token Identifier,
    tokenContent: optional token Literal
) {
    kwOptional, kwToken, id, tokenContent
};

rule ("parse_token_def":"TokenDef") DefineTokenTypeStatement(
    kwExtract: optional token Keyword"extract",
    _: token Keyword"token",
    (Token) tokenType: token Identifier
) {
    kwExtract, tokenType
};

rule ("parse_token_ignore_rule":"TokenRule")
DefineIgnoreTokenRuleStatement(_: token Keyword"ignore", value: LiteralOrRegExp) value;

rule ("parse_token_rule":"TokenRule") DefineTokenRuleStatement((Token) tokenType: token Identifier, value: LiteralOrRegExp) {
    tokenType, value
};

rule LiteralOrRegExp = TokenLiteral | TokenRegExp;
rule TokenLiteral(t: token Literal) t;
rule TokenRegExp(t: token RegExp) t;

rule ("parse_semantic":"String") DefineSemanticStatement(_: token Keyword"semantic", (Semantic) id: token Identifier) id;

rule ("parse_expr":"Expr") Expression = ConcatExpression | DictExpression;

rule ConcatExpression(
    (Variable) first: token Identifier,
    rest: optional ConcatExpressionTail
) first | rest;
rule ConcatExpressionTail(
    _: token Symbol"|",
    rest: ConcatExpression
) rest;

rule DictExpression(
    _: token Symbol"{",
    values: optional VariableList,
    _: token Symbol"}"
) values;

rule VariableList((Variable) first: token Identifier, rest: optional VariableListTail) first | rest;
rule VariableListTail(_: token Symbol",", rest: optional VariableList) rest;
