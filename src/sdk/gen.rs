// Macros used to generate SDK code

/// Generate basic functionality
#[macro_export]
macro_rules! generate_base {
    () => {
        use regen::sdk::RegenError;
        use regen::sdk::util::ParseHook;
        use std::collections::VecDeque;


        /// Convinience macro for parsing an optional parameter in a functional derivation
        macro_rules! optional {
            ( $ts:ident, $inner:expr ) => {
                // save the pos to restore in case of failure
                if !$ts.push() {
                    None
                } else {
                    let ast = $inner;
                    if ast.is_none() {
                        // restore if failure
                        $ts.set_error(false);
                        $ts.restore();
                    }
                    // remove the saved pos
                    $ts.pop();
                    ast
                }
            };

        }

        /// Convinience macro for parsing a token in a functional derivation
        // macro_rules! token {
        //     ($param_type_name:ident :: parse ( $ts:ident ) ) => {
        //         $ts.consume().filter(|token| {
        //             match &token.token_type {
        //                 Tokens::$param_type_name => true,
        //                 _ => false,
        //             }
        //         })

        //     };
        //     ($param_type_name:ident :: $lit:literal ( $ts:ident ) ) => {
        //         $ts.consume().filter(|token| {
        //             if let Tokens::$param_type_name = &token.token_type {
        //                 if &token.value == $lit {
        //                     return true;
        //                 }
        //             }
        //             false
        //         })
        //     };
        // }
        macro_rules! token {
            ($param_type_name:ident :: parse ( $ts:ident ) ) => {
                {
                    $ts.consume().filter(|token| {
                        match &token.token_type {
                            Tokens::$param_type_name => true,
                            _ => false,
                        }
                    }).cloned().or_else(|| {
                        $ts.set_error(false);
                        None
                    })
                }

            };
            ($param_type_name:ident :: $lit:literal ( $ts:ident ) ) => {
                {
                    $ts.consume().filter(|token| {
                        if let Tokens::$param_type_name = &token.token_type {
                            if &token.value == $lit {
                                return true;
                            }
                        }
                        false
                    }).cloned()
                    .or_else(|| {
                        $ts.set_error(false);
                        None
                    })
                }
            };
        }
    };
}

/// Macro to define functions and objects related to tokens (Token, TokenizerAction, TokenStream and fn tokenize)
/// Each token consists of its type, its literal value and its position in the input text
#[macro_export]
macro_rules! generate_token_sdk {
    (
        tokens: [ $( $token_type:ident ),* , ];
        regex: [ $( $token_regex:ident = $token_regex_literal:literal ),* , ];
        rules: [ $( [ $( $token_rule:tt ),* ] ),* , ];
    ) => {
        use regex::Regex;
        /// Token type enum
        #[derive(Debug, Clone)]
        pub enum Tokens { 
            /// Internal token type used to mark unrecognized tokens
            Unknown, 
            $( $token_type ),*
        }

        /// Token data generated by tokenizer to be used in semantic analysis and AST generation
        #[derive(Debug, Clone)]
        pub struct Token {
            /// Token type
            token_type: Tokens,
            /// Token value
            value: String,
            /// Token absolute position in the input text
            /// (start, end) - start is inclusive, end is exclusive
            pub pos: (usize, usize),
        }

        /// Action the tokenizer can take at each step
        enum TokenizerAction {
            /// Panic when a token cannot be matched.
            /// 
            /// The tokenizer will skip the next character, mark it as unrecognized and try to match again.
            Panic,
            /// Keep the token
            /// If should extract, the token will be saved in a second list that doesn't get passed into AST generation
            Keep(bool /* should extract */, Token),
            /// Discoard the token (for example, whitespaces)
            Ignore
        }

        /// A stream of tokens with support for backtracking for AST generation
        struct TokenStream<'t> {
            /// Token data
            tokens: &'t[Token],
            /// Current token index
            index: usize,
            /// Position stack
            stack: Vec<usize>,
            /// Max stack size
            max_stack_size: usize,
            /// Best guess at which token is causing a syntax error
            best_error_guess: usize
        }

        impl<'t> TokenStream<'t> {
            /// Create a new TokenStream
            fn new(tokens:&'t[Token], max_stack_size: usize) -> Self {
                Self {
                    tokens,
                    index: 0,
                    stack: Vec::new(),
                    max_stack_size,
                    best_error_guess: 0
                }
            }

            /// Returns if there is no token left after the current position
            fn is_exhausted(&self) -> bool {
                self.index >= self.tokens.len()
            }

            /// Get the best guess at which token is causing a syntax error
            fn get_guess_err_token(&self) -> Option<&'t Token> {
                self.tokens.get(self.best_error_guess).or(self.tokens.get(self.index))
            }

            /// Set the error guess at the current index
            fn set_error(&mut self, force: bool) {
                if force || self.index > self.best_error_guess {
                    self.best_error_guess = self.index;
                }
            }

            /// Returns the next token if available, and advance the position
            /// A reference is returned to avoid copying the token
            fn consume(&mut self) -> Option<&'t Token> {
                match self.tokens.get(self.index) {
                    Some(token) => {
                        self.index += 1;
                        Some(&token)
                    }
                    None => None,
                }
            }          

            /// Push the current position to stack so it can be restored
            fn push(&mut self) -> bool {
                if self.stack.len() >= self.max_stack_size {
                    return false;
                }
                self.stack.push(self.index);
                true
            }

            /// Pop position stack without restoring the position
            fn pop(&mut self) {
                self.stack.pop();
            }

            /// Restore the position to be the index on the stack top.
            /// This does not pop the stack.
            fn restore(&mut self) {
                self.index = *self.stack.last().unwrap();
            }
        }

        /// Macro that defines a tokenizer rule. This is used inside the tokenizer function
        /// 
        /// A tokenizer rule matches a literal or a regex pattern at the start of the input.
        /// Then it sets `$current_len` and `$action` accordingly.
        /// The rule will only be applied if the length of the matched string is greater than `$current_len`.
        macro_rules! tokenizer_rule {
            // Literal match rule
            ($current_len:ident, $action:ident, $rest:ident, $index:ident, $should_extract:expr, $match_token_type:ident, $literal:literal, $len:expr,) => {
                if $len > $current_len && $rest.starts_with($literal) {
                    $action = TokenizerAction::Keep($should_extract, Token {
                        token_type: Tokens :: $match_token_type,
                        value: $literal.to_owned(),
                        pos: ( $index, $index + $len ),
                    });
                    $current_len = $len;
                }
            };
            // Regex match rule
            // Note that regexes are precompiled before the loop, so an identity is used
            ($current_len:ident, $action:ident, $rest:ident, $index:ident, $should_extract:expr, $match_token_type:ident, $re:ident,) => {
                if let Some(m) = $re.find($rest) {
                    let m_str = m.as_str();
                    let len = m_str.len();
                    if len > $current_len {
                        $action = TokenizerAction::Keep($should_extract, Token {
                            token_type: Tokens :: $match_token_type,
                            value: m_str.to_owned(),
                            pos: ( $index, $index + len ),
                        });
                        $current_len = len;
                    }
                }
            };
            // Literal ignore rule
            ($current_len:ident, $action:ident, $rest:ident, $index:ident, $literal:literal, $len:expr,) => {
                if $len > $current_len && $rest.starts_with($literal) {
                    $action = TokenizerAction::Ignore;
                    $current_len = $len;
                }
            };
            // Regex ignore rule
            ($current_len:ident, $action:ident, $rest:ident, $index:ident, $re:ident,) => {
                if let Some(m) = $re.find($rest) {
                    let len = m.end();
                    if len > $current_len {
                        $action = TokenizerAction::Ignore;
                        $current_len = len;
                    }
                }
            };
        }
    
        /// The tokenizer function
        fn tokenize_internal(input: &str) -> (Vec<Token>, Vec<Token>, Vec<Token>) {
            // Compile regex rules
            $(
                let $token_regex = Regex::new($token_regex_literal).unwrap();
            )*
            let mut tokens = Vec::new();
            let mut extracted_tokens = Vec::new();
            let mut unrecognized_tokens = Vec::new();
            let mut index = 0;
            while index < input.len() {
                // Get the current slice
                let rest = &input[index..];
                let mut action = TokenizerAction::Panic;
                let mut current_len = 0;
                // Run rules
                $(
                    tokenizer_rule!(
                        current_len,
                        action,
                        rest,
                        index,
                        $(
                            $token_rule,
                        )*
                    );
                )*
                // Process action
                match action {
                    TokenizerAction::Panic => {
                        // Unrecognized token, skip one character
                        unrecognized_tokens.push(Token {
                            token_type: Tokens::Unknown,
                            value: rest[0..1].to_owned(),
                            pos: (index, index+1),
                        });
                        index+=1;
                    },
                    TokenizerAction::Keep(should_extract, token) => {
                        // Recognized token
                        if should_extract {
                            extracted_tokens.push(token);
                        } else {
                            tokens.push(token);
                        }
                        index += current_len;
                    },
                    TokenizerAction::Ignore => {
                        // Ignore token
                        index += current_len;
                    }
                }
            }
            (tokens, extracted_tokens, unrecognized_tokens)
        }
    };
}

/// Macro used to generate semantic related function and objects (Semantics, SemBlock, SemInfo, Span)
#[macro_export]
macro_rules! generate_semantic_sdk {
    [ $( $sem_type:ident ),* ,] => {
        use html_escape;

        /// Semantic type enum
        #[derive(Debug, Clone)]
        pub enum Semantics {
            /// Built-in semantic type that corresponds to a token
            Token(Tokens),
            /// Semantic type with extra tags.
            /// The tags are strings and are directly converted to HTMl classes when generating html code blocks.
            Tag(String, Box<Semantics>),
            $( $sem_type ),*
        }

        impl Semantics{
            fn to_html_class(&self) -> String {
                match self {
                    Semantics::Token(token_type) => format!("token {:?}", token_type),
                    Semantics::Tag(tag, underlying) => format!("{tag} {}", underlying.to_html_class()),
                    _ => format!("semantic token {:?}", self),
                }
            }
        }

        /// A span is part of the input source code marked with a semantic type.
        pub struct Span {
            /// The semantic type of the span
            pub semantic_type: Option<Semantics>,
            /// The text of the span
            pub text: String,
        }
        
        impl Span {
            /// Convert the span to an html string, with proper espaces
            pub fn to_html(&self) -> String {
                let text = html_escape::encode_text(&self.text).to_string();
                match &self.semantic_type {
                    None => text,
                    Some(semantic_type) => {
                        format!(r#"<span class="{cls}">{text}</span>"#, cls=semantic_type.to_html_class())
                    }
                }
            }
        }

        /// Semantic block used internally by SemInfo to mark a range of text with a semantic
        struct SemBlock{
            /// Semantic type
            semantic_type: Semantics,
            /// Token absolute position - start and end
            pos: (usize, usize),
        }

        /// Semantic information that is stored and updated during the parsing process
        pub struct SemInfo {
            content: String,
            blocks: Vec<SemBlock>,
        }
        
        impl SemInfo {
            /// Create a new SemInfo
            fn new(content: &str) -> Self {
                Self { blocks: Vec::new(), content: content.to_owned() }
            }
        
            /// Insert all tokens and mark them with the associated token semantic
            pub fn insert_all(&mut self, tokens: &[Token]) {
                for t in tokens {
                    self.blocks.push(SemBlock {
                        semantic_type: Semantics::Token(t.token_type.clone()),
                        pos: t.pos,
                    });
                }
                self.blocks.sort_by(|a, b| {
                    a.pos.0.cmp(&b.pos.0)
                })
            }
        
            /// Set the semantic of a token
            pub fn set(&mut self, token: &Token, semantic_type: Semantics) {
                let result = self.blocks.binary_search_by(|probe| {
                    probe.pos.0.cmp(&token.pos.0)
                });
                match result {
                    Ok(index) => {
                        
                        self.blocks[index].semantic_type = semantic_type;
                    },
                    Err(index) => {
                        self.blocks.insert(index, SemBlock {
                            semantic_type,
                            pos: token.pos,
                        });
                    }
                }
            }
        
            /// Break down input content into spans marked with semantics
            pub fn get_spans(&self) -> Vec<Span> {
                let mut code_blocks = Vec::new();
                let mut cur = 0;
                for semantic_token in &self.blocks {
                    let (start, end) = semantic_token.pos;
                    if start > cur {
                        code_blocks.push(Span {
                            semantic_type: None,
                            text: self.content[cur..start].to_owned(),
                        });
                    }
                    cur = cur.max(start);
                    if end > cur {
                        code_blocks.push(Span {
                            semantic_type: Some(semantic_token.semantic_type.clone()),
                            text: self.content[cur..end].to_owned(),
                        });
                        cur = end;
                    }
                }
                if cur < self.content.len() {
                    code_blocks.push(Span {
                        semantic_type: None,
                        text: self.content[cur..].to_owned(),
                    });
                }
                code_blocks
            }

            pub fn get_html(&self) -> String {
                let mut html = String::new();
                for block in self.get_spans() {
                    html.push_str(&block.to_html());
                }
                html
            }
        }
    };
}

/// Generate the entry point APIs for the SDK
#[macro_export]
macro_rules! generate_api {
    ($pt_target:ident, $ast_target:ident, $tokenize_internal:ident) => {
        pub struct Context {
            pub si: SemInfo,
            pub err: Vec<RegenError>,
            pub tokens: Vec<Token>,
        }

        impl Context {
            pub fn ast_one(mut self, stack_size: usize) -> ContextWithAST {
                let mut ts = TokenStream::new(&self.tokens, stack_size);
                let mut asts = Vec::new();
                let ast = $ast_target::parse(&mut ts);
                if let Some(ast) = ast {
                    ast.apply_semantic(&mut self.si);
                    asts.push(ast);
                }
                ContextWithAST {
                    si: self.si,
                    err: self.err,
                    tokens: self.tokens,
                    asts
                }
            }
            pub fn ast_all(self, stack_size: usize) -> Result<ContextWithAST, Vec<RegenError>> {
                let ctx = self.ast_all_unchecked(stack_size);
                if ctx.err.is_empty() {
                    Ok(ctx)
                } else {
                    Err(ctx.err)
                }
            }

            pub fn ast_all_unchecked(mut self, stack_size: usize) -> ContextWithAST {
                let mut ts = TokenStream::new(&self.tokens, stack_size);
                let mut asts = Vec::new();
                let mut last_failed = false;

                loop {
                    match $ast_target::parse(&mut ts) {
                        Some(ast) => {
                            ast.apply_semantic(&mut self.si);
                            last_failed = false;
                            asts.push(ast);
                            if ts.is_exhausted() {
                                break;
                            }
                        },
                        None => {
                            if let Some(token) = ts.consume() {
                                
                                if !last_failed {
                                    last_failed = true;
                                    if let Some(err_token) = ts.get_guess_err_token() {
                                        self.err.push(regen::err!(err_token, "Syntax error near this location".to_owned()));
                                    }else{
                                        
                                        self.err.push(regen::err!(token, "Syntax error".to_owned()));
                                    }
                                }
                            } else {
                                break;
                            }
                        }
                    }
                }

                ContextWithAST {
                    si: self.si,
                    err: self.err,
                    tokens: self.tokens,
                    asts
                }
            }
        }

        pub struct ContextWithAST {
            pub si: SemInfo,
            pub err: Vec<RegenError>,
            pub tokens: Vec<Token>,
            pub asts: Vec<$ast_target>,
        }

        impl ContextWithAST {
            pub fn parse<'a>(&'a mut self) -> Result<Vec<$pt_target<'a>>, &'a [RegenError]> {
                let pts = self.asts.iter().map(|ast| {
                    $pt_target::from_ast(ast, &mut self.si, &mut self.err)
                }).collect();

                if self.err.is_empty() {
                    Ok(pts)
                } else {
                    Err(&self.err)
                }
            }
            pub fn parse_unchecked<'a>(&'a mut self) -> Vec<$pt_target<'a>> {
                self.asts.iter().map(|ast| {
                    $pt_target::from_ast(ast, &mut self.si, &mut self.err)
                }).collect()
            }
        }

        /// Tokenize the input and return the Context, which can be used to generate AST
        pub fn tokenize(content: &str) -> Context {
            let mut si = SemInfo::new(content);
            let (tokens, extract_tokens, unrec_tokens) = $tokenize_internal(content);
            si.insert_all(&tokens);
            si.insert_all(&extract_tokens);
            si.insert_all(&unrec_tokens);
            Context {
                si,
                err: vec![],
                tokens,
            }
        }

    };
}

/// Generate a union enum definition (for both AST and PT)
#[macro_export]
macro_rules! generate_union {
    ($type_name:ident, $( $derivation_type_name:ident, )*) => {
        /// Generated union type
        #[derive(Debug)]
        pub enum $type_name {
            $( $derivation_type_name(Box<$derivation_type_name>), )*
        }
    };
}

#[macro_export]
macro_rules! generate_ast_union {
    ($ast_type_name:ident, $( $derivation_type_name:ident, )*) => {
        #[derive(Debug)]
        pub enum $ast_type_name {
            $( $derivation_type_name(Box<$derivation_type_name>), )*
        }
        impl $ast_type_name {
            fn parse(ts: & mut TokenStream) -> Option<Self> {
                if !ts.push() { return None; };
                $(
                    if let Some(r) = $derivation_type_name::parse(ts) {
                        ts.pop();
                        return Some($ast_type_name::$derivation_type_name(Box::new(r)))
                    }
                    ts.set_error(false);
                    ts.restore();
                )*
                ts.pop();
                return None;
            }
            fn apply_semantic(&self, si: &mut SemInfo) {
                match self {
                    $( $ast_type_name::$derivation_type_name(r) => r.apply_semantic(si), )*
                }
            }
        }
    };
}

/// Generate union implementation for a PT union type
#[macro_export]
macro_rules! generate_pt_union_impl {
    ($from_ast:ident, $ast_type_name:ident, $pt_type_name:ident, $( $pt_hooked_type_name:ident, $pt_derivation_type_name:ident, $ast_derivation_type_name:ident, )*) => {
        impl<'p> $pt_type_name<'p> {
            fn $from_ast(ast: &'p $ast_type_name, semantic_info: &mut SemInfo, errors: &mut Vec<RegenError>) -> Self {
                match ast {
                    $(
                        $ast_type_name::$ast_derivation_type_name(ast) => {
                            return Self::$pt_hooked_type_name(Box::new($pt_derivation_type_name::from_ast(ast, semantic_info, errors)));
                        }
                    )*
                }
            }
        }
    };
    ($from_ast:ident, $ast_type_name:ident, $pt_type_name:ident, $( $pt_hooked_type_name:ident, $pt_derivation_type_name:ident, $ast_derivation_type_name:ident, )* NO_LIFETIME) => {
        impl $pt_type_name {
            fn $from_ast(ast: &$ast_type_name, semantic_info: &mut SemInfo, errors: &mut Vec<RegenError>) -> Self {
                match ast {
                    $(
                        $ast_type_name::$ast_derivation_type_name(ast) => {
                            return $pt_type_name::$pt_hooked_type_name(Box::new($pt_derivation_type_name::from_ast(ast, semantic_info, errors)));
                        }
                    )*
                }
            }
        }
    };
}

#[macro_export]
macro_rules! pt_union_impl {
    (
        $ast:ident, $si:ident, $err:ident,
        [ $ast_type_name:ident, $( $ast_derivation_type_name:ident, )* ],
        [ $pt_type_name:ty, $( $pt_hooked_type_name:ty, $pt_derivation_type_name:ident, )* ]
    ) => {
        match $ast {
            $(
                $ast_type_name::$ast_derivation_type_name(ast) => {
                    return Self::$pt_derivation_type_name(Box::new($pt_derivation_type_name::from_ast(ast, $si, $err)));
                }
            )*
        }
    }
}

#[macro_export]
macro_rules! generate_union_impl {
    (
        $from_ast:ident,
        [ $ast_type_name:ident, $( $ast_derivation_type_name:ident, )* ],
        [ $pt_type_name:ty, $( $pt_hooked_type_name:ty, $pt_derivation_type_name:ident, )* ],
    ) => {
        regen::generate_ast_union!($ast_type_name, $( $ast_derivation_type_name, )* );
        impl<'p> $pt_type_name {
            fn $from_ast(ast: &'p $ast_type_name, si: &mut SemInfo, err: &mut Vec<RegenError>) -> Self {
                regen::pt_union_impl!(
                    ast, si, err,
                    [ $ast_type_name, $( $ast_derivation_type_name, )* ],
                    [ $pt_type_name, $( $pt_hooked_type_name, $pt_derivation_type_name, )* ]
                );
            }
        }
    };
    (
        $from_ast:ident,
        [ $ast_type_name:ident, $( $ast_derivation_type_name:ident, )* ],
        [ $pt_type_name:ident, $( $pt_hooked_type_name:ident, $pt_derivation_type_name:ident, )* ],
        no_lifetime
    ) => {
        regen::generate_ast_union!($ast_type_name, $( $ast_derivation_type_name, )* );
        impl $pt_type_name {
            fn $from_ast(ast: &$ast_type_name, si: &mut SemInfo, err: &mut Vec<RegenError>) -> Self {
                regen::pt_union_impl!(
                    ast, si, err,
                    [ $ast_type_name, $( $ast_derivation_type_name, )* ],
                    [ $pt_type_name, $( $pt_hooked_type_name, $pt_derivation_type_name, )* ]
                );
            }
        }
    };
}